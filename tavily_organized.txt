# Tavily API Documentation

## 1. Quickstart

Start searching with Tavily in under 5 minutes. Get your free Tavily API key by heading to the Tavily Platform and signing in (or creating an account). Then, copy one of your API keys from your dashboard.

### 1.1. Install the Tavily SDK

Install the Tavily SDK in your language of choice. For Node.js, use:
```bash
npm i @tavily/core
```

### 1.2. Start Searching with Tavily

Run your first Tavily Search in 4 lines of code. Simply replace the API key in this snippet with your own.

```javascript
const { tavily } = require("@tavily/core");

const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.search("Who is Leo Messi?");

console.log(response);
```

### 1.3. Next Steps

That’s all it takes to start using Tavily’s basic features! If you want to learn how to implement more complex workflows in Python, check out our intermediate-level Getting Started notebook. Or, dive deep into our API and read about the different parameters on our API Reference page, and learn how to integrate natively with one of our SDKs.

## 2. Credits & Pricing

Learn how to get and manage your Tavily API Credits.

### 2.1. Free API Credits

You get 1,000 free API Credits every month. No credit card required.

### 2.2. Pricing Overview

Tavily operates on a simple, credit-based model:

- **Free**: 1,000 credits/month
- **Pay-as-you-go**: $0.008 per credit (allows you to be charged per credit once your plan’s credit limit is reached).
- **Monthly plans**: $0.0075 - $0.005 per credit
- **Enterprise**: Custom pricing and volume

| Plan         | Credits per month | Monthly price | Price per credit |
|--------------|-------------------|---------------|------------------|
| Researcher   | 1,000             | Free          | -                |
| Project      | 4,000             | $30           | $0.0075          |
| Bootstrap    | 15,000            | $100          | $0.0067          |
| Startup      | 38,000            | $220          | $0.0058          |
| Growth       | 100,000           | $500          | $0.005           |
| Pay as you go| Per usage         | $0.008 / Credit | $0.008         |
| Enterprise   | Custom            | Custom        | Custom           |

Head to billing to explore our different options and manage your plan.

### 2.3. API Credits Costs

#### 2.3.1. Tavily Search

Your search depth determines the cost of your request.

- **Basic Search (`basic`):** Each request costs 1 API credit.
- **Advanced Search (`advanced`):** Each request costs 2 API credits.

#### 2.3.2. Tavily Extract

The number of successful URL extractions and your extraction depth determines the cost of your request. You never get charged if a URL extraction fails.

- **Basic Extract (`basic`):** Every 5 successful URL extractions cost 1 API credit.
- **Advanced Extract (`advanced`):** Every 5 successful URL extractions cost 2 API credits.

#### 2.3.3. Tavily Map

The number of pages mapped and whether or not natural-language instructions are specified determines the cost of your request. You never get charged if a map request fails.

- **Regular Mapping:** Every 10 successful pages returned cost 1 API credit.
- **Map with (instructions):** Every 10 successful pages returned cost 2 API credits.

#### 2.3.4. Tavily Crawl

Tavily Crawl combines both mapping and extraction operations, so the cost is the sum of both: Crawl Cost = Mapping Cost + Extraction Cost.

For example:
- If you crawl 10 pages with basic extraction depth, you’ll be charged 1 credit for mapping (10 pages) + 2 credits for extraction (10 successful extractions ÷ 5) = 3 total credits.
- If you crawl 10 pages with advanced extraction depth, you’ll be charged 1 credit for mapping + 4 credits for extraction = 5 total credits.

## 3. Rate Limits

Learn about Tavily’s API rate limits for both development and production environments. We offer two types of rate limits based on the environment associated with your API key. Create your Development or Production API keys.

| Environment | Requests per minute (RPM) |
|-------------|---------------------------|
| Development | 100                       |
| Production  | 1,000                     |

## 4. API Reference

### 4.1. Base URL

The base URL for all requests to the Tavily API is: `https://api.tavily.com`

### 4.2. Authentication

All Tavily endpoints are authenticated using API keys. Get your free API key.

Example `curl` command for authentication:
```bash
curl -X POST https://api.tavily.com/search \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer tvly-YOUR_API_KEY" \
  -d '{"query": "Who is Leo Messi?"}'
```

### 4.3. Tavily Search API

Execute a search query using Tavily Search. `POST /search`

**Authorizations:**
- `Authorization` (string, header, required): Bearer authentication header in the form `Bearer <token>`, where `<token>` is your Tavily API key (e.g., `Bearer tvly-YOUR_API_KEY`).

**Body (`application/json`) Parameters:**
- `query` (string, required): The search query to execute with Tavily. Example: "who is Leo Messi?"
- `auto_parameters` (boolean, default: `false`): When enabled, Tavily automatically configures search parameters based on your query's content and intent. You can still set other parameters manually, and your explicit values will override the automatic ones. The parameters `include_answer`, `include_raw_content`, and `max_results` must always be set manually, as they directly affect response size. Note: `search_depth` may be automatically set to `advanced` when it's likely to improve results. This uses 2 API credits per request. To avoid the extra cost, you can explicitly set `search_depth` to `basic`. Currently in BETA.
- `topic` (enum<string>, default: `general`): The category of the search. `news` is useful for retrieving real-time updates, particularly about politics, sports, and major current events covered by mainstream media sources. `general` is for broader, more general-purpose searches that may include a wide range of sources. Available options: `general`, `news`.
- `search_depth` (enum<string>, default: `basic`): The depth of the search. `advanced` search is tailored to retrieve the most relevant sources and content snippets for your query, while `basic` search provides generic content snippets from each source. A basic search costs 1 API Credit, while an advanced search costs 2 API Credits. Available options: `basic`, `advanced`.
- `chunks_per_source` (integer, default: `3`): Chunks are short content snippets (maximum 500 characters each) pulled directly from the source. Use `chunks_per_source` to define the maximum number of relevant chunks returned per source and to control the content length. Chunks will appear in the content field as: `<chunk 1> [...] <chunk 2> [...] <chunk 3>`. Available only when `search_depth` is `advanced`. Required range: 1 <= x <= 3.
- `max_results` (integer, default: `5`): The maximum number of search results to return. Required range: 0 <= x <= 20. Example: 1.
- `time_range` (enum<string>): The time range back from the current date to filter results. Useful when looking for sources that have published data. Available options: `day`, `week`, `month`, `year`, `d`, `w`, `m`, `y`.
- `days` (integer, default: `7`): Number of days back from the current date to include. Available only if `topic` is `news`. Required range: x >= 1.
- `include_answer` (boolean, default: `false`): Include an LLM-generated answer to the provided query. `basic` or `true` returns a quick answer. `advanced` returns a more detailed answer.
- `include_raw_content` (boolean, default: `false`): Include the cleaned and parsed HTML content of each search result. `markdown` or `true` returns search result content in markdown format. `text` returns the plain text from the results and may increase latency.
- `include_images` (boolean, default: `false`): Also perform an image search and include the results in the response.
- `include_image_descriptions` (boolean, default: `false`): When `include_images` is `true`, also add a descriptive text for each image.
- `include_favicon` (boolean, default: `false`): Whether to include the favicon URL for each result.
- `include_domains` (string[]): A list of domains to specifically include in the search results.
- `exclude_domains` (string[]): A list of domains to specifically exclude from the search results.
- `country` (enum<string>): Boost search results from a specific country. This will prioritize content from the selected country in the search results. Available only if `topic` is `general`. Available options: (list of countries).

**Response (200 `application/json`):** Search results returned successfully.
- `query` (string, required): The search query that was executed. Example: "Who is Leo Messi?"
- `answer` (string, required): A short answer to the user's query, generated by an LLM. Included in the response only if `include_answer` is requested (i.e., set to `true`, `basic`, or `advanced`). Example: "Lionel Messi, born in 1987, is an Argentine footballer widely regarded as one of the greatest players of his generation. He spent the majority of his career playing for FC Barcelona, where he won numerous domestic league titles and UEFA Champions League titles. Messi is known for his exceptional dribbling skills, vision, and goal-scoring ability. He has won multiple FIFA Ballon d'Or awards, numerous La Liga titles with Barcelona, and holds the record for most goals scored in a calendar year. In 2014, he led Argentina to the World Cup final, and in 2015, he helped Barcelona capture another treble. Despite turning 36 in June, Messi remains highly influential in the sport."
- `images` (object[], required): List of query-related images. If `include_image_descriptions` is `true`, each item will have `url` and `description`. Example: `[]`.
- `results` (object[], required): A list of sorted search results, ranked by relevancy.
- `response_time` (number, required): Time in seconds it took to complete the request. Example: "1.67".
- `auto_parameters` (object): A dictionary of the selected `auto_parameters`, only shown when `auto_parameters` is `true`. Example: `{ "topic": "general", "search_depth": "basic" }`.

**JavaScript SDK Example for Tavily Search:**
```javascript
const { tavily } = require("@tavily/core");

const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.search("Who is Leo Messi?");

console.log(response);
```

**Example Response for Tavily Search:**
```json
{
  "query": "Who is Leo Messi?",
  "answer": "Lionel Messi, born in 1987, is an Argentine footballer widely regarded as one of the greatest players of his generation. He spent the majority of his career playing for FC Barcelona, where he won numerous domestic league titles and UEFA Champions League titles. Messi is known for his exceptional dribbling skills, vision, and goal-scoring ability. He has won multiple FIFA Ballon d'Or awards, numerous La Liga titles with Barcelona, and holds the record for most goals scored in a calendar year. In 2014, he led Argentina to the World Cup final, and in 2015, he helped Barcelona capture another treble. Despite turning 36 in June, Messi remains highly influential in the sport.",
  "images": [],
  "results": [
    {
      "title": "Lionel Messi Facts | Britannica",
      "url": "https://www.britannica.com/facts/Lionel-Messi",
      "content": "Lionel Messi, an Argentine footballer, is widely regarded as one of the greatest football players of his generation. Born in 1987, Messi spent the majority of his career playing for Barcelona, where he won numerous domestic league titles and UEFA Champions League titles. Messi is known for his exceptional dribbling skills, vision, and goal",
      "score": 0.81025416,
      "raw_content": null,
      "favicon": "https://britannica.com/favicon.png"
    }
  ],
  "auto_parameters": {
    "topic": "general",
    "search_depth": "basic"
  },
  "response_time": "1.67"
}
```

### 4.4. Tavily Extract API

Extract web page content from one or more specified URLs using Tavily Extract. `POST /extract`

**Authorizations:**
- `Authorization` (string, header, required): Bearer authentication header in the form `Bearer <token>`, where `<token>` is your Tavily API key (e.g., `Bearer tvly-YOUR_API_KEY`).

**Body (`application/json`) Parameters:**
- `urls` (string, required): The URL to extract content from. Example: "https://en.wikipedia.org/wiki/Artificial_intelligence"
- `include_images` (boolean, default: `false`): Include a list of images extracted from the URLs in the response. Default is `false`.
- `include_favicon` (boolean, default: `false`): Whether to include the favicon URL for each result.
- `extract_depth` (enum<string>, default: `basic`): The depth of the extraction process. `advanced` extraction retrieves more data, including tables and embedded content, with higher success but may increase latency. `basic` extraction costs 1 credit per 5 successful URL extractions, while `advanced` extraction costs 2 credits per 5 successful URL extractions. Available options: `basic`, `advanced`.
- `format` (enum<string>, default: `markdown`): The format of the extracted web page content. `markdown` returns content in markdown format. `text` returns plain text and may increase latency. Available options: `markdown`, `text`.

**Response (200 `application/json`):** Extraction results returned successfully.
- `results` (object[]): A list of extracted content from the provided URLs.
- `failed_results` (object[]): A list of URLs that could not be processed. Example: `[]`.
- `response_time` (number): Time in seconds it took to complete the request.

**JavaScript SDK Example for Tavily Extract:**
```javascript
const { tavily } = require("@tavily/core");
const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.extract("https://en.wikipedia.org/wiki/Artificial_intelligence");
console.log(response);
```

**Example Response for Tavily Extract:**
```json
{
  "results": [
    {
      "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "raw_content": "\"Jump to content\\nMain menu\\nSearch\\nAppearance\\nDonate\\nCreate account\\nLog in\\nPersonal tools\\n        Photograph your local culture, help Wikipedia and win!\\nToggle the table of contents\\nArtificial intelligence\\n161 languages\\nArticle\\nTalk\\nRead\\nView source\\nView history\\nTools\\nFrom Wikipedia, the free encyclopedia\\n\\"AI\\" redirects here. For other uses, see AI (disambiguation) and Artificial intelligence (disambiguation).\\nPart of a series on\\nArtificial intelligence (AI)\\nshow\\nMajor goals\\nshow\\nApproaches\\nshow\\nApplications\\nshow\\nPhilosophy\\nshow\\nHistory\\nshow\\nGlossary\\nvte\\nArtificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go)...................",
      "images": [],
      "favicon": "https://en.wikipedia.org/static/favicon/wikipedia.ico"
    }
  ],
  "failed_results": [],
  "response_time": 0.02
}
```

### 4.5. Tavily Crawl API (BETA)

Tavily Crawl is a graph-based website traversal tool that can explore hundreds of paths in parallel with built-in extraction and intelligent discovery. `POST /crawl`

**Authorizations:**
- `Authorization` (string, header, required): Bearer authentication header in the form `Bearer <token>`, where `<token>` is your Tavily API key (e.g., `Bearer tvly-YOUR_API_KEY`).

**Body (`application/json`) Parameters:**
- `url` (string, required): The root URL to begin the crawl. Example: "docs.tavily.com"
- `max_depth` (integer, default: `1`): Max depth of the crawl. Defines how far from the base URL the crawler can explore. Required range: x >= 1.
- `max_breadth` (integer, default: `20`): Max number of links to follow per level of the tree (i.e., per page). Required range: x >= 1.
- `limit` (integer, default: `50`): Total number of links the crawler will process before stopping. Required range: x >= 1.
- `instructions` (string): Natural language instructions for the crawler. When specified, the mapping cost increases to 2 API credits per 10 successful pages instead of 1 API credit per 10 pages. Example: "Python SDK".
- `select_paths` (string[]): Regex patterns to select only URLs with specific path patterns (e.g., `/docs/.*`, `/api/v1.*`).
- `select_domains` (string[]): Regex patterns to select crawling to specific domains or subdomains (e.g., `^docs\.example\.com$`).
- `exclude_paths` (string[]): Regex patterns to exclude URLs with specific path patterns (e.g., `/private/.*`, `/admin/.*`).
- `exclude_domains` (string[]): Regex patterns to exclude specific domains or subdomains from crawling (e.g., `^private\.example\.com$`).
- `allow_external` (boolean, default: `false`): Whether to allow following links that go to external domains.
- `include_images` (boolean, default: `false`): Whether to include images in the crawl results.
- `categories` (enum<string>[]): Filter URLs using predefined categories like 'Documentation', 'Blog', 'API', etc.
- `extract_depth` (enum<string>, default: `basic`): Advanced extraction retrieves more data, including tables and embedded content, with higher success but may increase latency. `basic` extraction costs 1 credit per 5 successful extractions, while `advanced` extraction costs 2 credits per 5 successful extractions. Available options: `basic`, `advanced`.
- `format` (enum<string>, default: `markdown`): The format of the extracted web page content. `markdown` returns content in markdown format. `text` returns plain text and may increase latency. Available options: `markdown`, `text`.
- `include_favicon` (boolean, default: `false`): Whether to include the favicon URL for each result.

**Response (200 `application/json`):** Crawl results returned successfully.
- `base_url` (string): The base URL that was crawled. Example: "docs.tavily.com"
- `results` (object[]): A list of extracted content from the crawled URLs.

**JavaScript SDK Example for Tavily Crawl:**
```javascript
const { tavily } = require("@tavily/core");

const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.crawl("https://docs.tavily.com", { instructions: "Find all pages on the Python SDK" });

console.log(response);
```

**Example Response for Tavily Crawl:**
```json
{
  "base_url": "docs.tavily.com",
  "results": [
    {
      "url": "https://docs.tavily.com/welcome",
      "raw_content": "Welcome - Tavily Docs\n\n[Tavily Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/dark.svg)](https://tavily.com/)\n\nSearch or ask...\n\nCtrl K\n\n- [Support](mailto:support@tavily.com)\n- [Get an API key](https://app.tavily.com)\n- [Get an API key](https://app.tavily.com)\n\nSearch...\n\nNavigation\n\n[Home](/welcome)[Documentation](/documentation/about)[SDKs](/sdk/python/quick-start)[Examples](/examples/use-cases/data-enrichment)[FAQ](/faq/faq)\n\nExplore our docs\n\nYour journey to state-of-the-art web search starts right here.\n\n[## Quickstart\n\nStart searching with Tavily in minutes](documentation/quickstart)[## API Reference\n\nStart using Tavily's powerful APIs](documentation/api-reference/endpoint/search)[## API Credits Overview\n\nLearn how to get and manage your Tavily API Credits](documentation/api-credits)[## Rate Limits\n\nLearn about Tavily's API rate limits for both development and production environments](documentation/rate-limits)[## Python\n\nGet started with our Python SDK, `tavily-python`](sdk/python/quick-start)[## Playground\n\nExplore Tavily's APIs with our interactive playground](https://app.tavily.com/playground)",
      "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/tavilyai/_generated/favicon/apple-touch-icon.png?v=3"
    },
    {
      "url": "https://docs.tavily.com/documentation/api-credits",
      "raw_content": "Credits & Pricing - Tavily Docs\n\n[Tavily Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/dark.svg)](https://tavily.com/)\n\nSearch or ask...\n\nCtrl K\n\n- [Support](mailto:support@tavily.com)\n- [Get an API key](https://app.tavily.com)\n- [Get an API key](https://app.tavily.com)\n\nSearch...\n\nNavigation\n\nOverview\n\nCredits & Pricing\n\n[Home](/welcome)[Documentation](/documentation/about)[SDKs](/sdk/python/quick-start)[Examples](/examples/use-cases/data-enrichment)[FAQ](/faq/faq)\n\n- [API Playground](https://app.tavily.com/playground)\n- [Community](https://community.tavily.com)\n- [Blog](https://blog.tavily.com)\n\n##### Overview\n\n- [About](/documentation/about)\n- [Quickstart](/documentation/quickstart)\n- [Credits & Pricing](/documentation/api-credits)\n- [Rate Limits](/documentation/rate-limits)\n\n##### API Reference\n\n- [Introduction](/documentation/api-reference/introduction)\n- [POST\n\n  Tavily Search](/documentation/api-reference/endpoint/search)\n- [POST\n\n  Tavily Extract](/documentation/api-reference/endpoint/extract)\n- [POST\n\n  Tavily Crawl](/documentation/api-reference/endpoint/crawl)\n- [POST\n\n  Tavily Map](/documentation/api-reference/endpoint/map)\n\n##### Best Practices\n\n- [Best Practices for Search](/documentation/best-practices/best-practices-search)\n- [Best Practices for Extract](/documentation/best-practices/best-practices-extract)\n\n##### Tavily MCP Server\n\n- [Tavily MCP Server](/documentation/mcp)\n\n##### Integrations\n\n- [LangChain](/documentation/integrations/langchain)\n- [LlamaIndex](/documentation/integrations/llamaindex)\n- [Zapier](/documentation/integrations/zapier)\n- [Dify](/documentation/integrations/dify)\n- [Composio](/documentation/integrations/composio)\n- [Make](/documentation/integrations/make)\n- [Agno](/documentation/integrations/agno)\n- [Pydantic AI](/documentation/integrations/pydantic-ai)\n- [FlowiseAI](/documentation/integrations/flowise)\n\n##### Legal\n\n- [Security & Compliance](https://trust.tavily.com)\n- [Privacy Policy](https://tavily.com/privacy)\n\n##### Help\n\n- [Help Center](https://help.tavily.com)\n\n##### Tavily Search Crawler\n\n- [Tavily Search Crawler](/documentation/search-crawler)\n\nOverview\n\n# Credits & Pricing\n\nLearn how to get and manage your Tavily API Credits.\n\n## [\u8203 ](#free-api-credits) Free API Credits\n\n[## Get your free API key\n\nYou get 1,000 free API Credits every month.\n**No credit card required.**](https://app.tavily.com)\n\n## [\u8203 ](#pricing-overview) Pricing Overview\n\nTavily operates on a simple, credit-based model:\n\n- **Free**: 1,000 credits/month\n- **Pay-as-you-go**: $0.008 per credit (allows you to be charged per credit once your plan's credit limit is reached).\n- **Monthly plans**: $0.0075 - $0.005 per credit\n- **Enterprise**: Custom pricing and volume\n\n| **Plan** | **Credits per month** | **Monthly price** | **Price per credit** |\n| --- | --- | --- | --- |\n| **Researcher** | 1,000 | Free | - |\n| **Project** | 4,000 | $30 | $0.0075 |\n| **Bootstrap** | 15,000 | $100 | $0.0067 |\n| **Startup** | 38,000 | $220 | $0.0058 |\n| **Growth** | 100,000 | $500 | $0.005 |\n| **Pay as you go** | Per usage | $0.008 / Credit | $0.008 |\n| **Enterprise** | Custom | Custom | Custom |\n\nHead to [my plan](https://app.tavily.com/account/plan) to explore our different options and manage your plan.\n\n## [\u8203 ](#api-credits-costs) API Credits Costs\n\n### [\u8203 ](#tavily-search) Tavily Search\n\nYour [search depth](/api-reference/endpoint/search#body-search-depth) determines the cost of your request.\n\n- **Basic Search (`basic`):**\n  Each request costs **1 API credit**.\n- **Advanced Search (`advanced`):**\n  Each request costs **2 API credits**.\n\n### [\u8203 ](#tavily-extract) Tavily Extract\n\nThe number of successful URL extractions and your [extraction depth](/api-reference/endpoint/extract#body-extract-depth) determines the cost of your request. You never get charged if a URL extraction fails.\n\n- **Basic Extract (`basic`):**\n  Every 5 successful URL extractions cost **1 API credit**.\n- **Advanced Extract (`advanced`):**\n  Every 5 successful URL extractions cost **2 API credits**.\n\n[Quickstart](/documentation/quickstart)[Rate Limits](/documentation/rate-limits)\n\n[x](https://x.com/tavilyai)[github](https://github.com/tavily-ai)[linkedin](https://linkedin.com/company/tavily)[website](https://tavily.com)\n\n[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=docs&utm_source=docs.tavily.com)\n\nOn this page\n\n- [Free API Credits](#free-api-credits)\n- [Pricing Overview](#pricing-overview)\n- [API Credits Costs](#api-credits-costs)\n- [Tavily Search](#tavily-search)\n- [Tavily Extract](#tavily-extract)",
      "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/tavilyai/_generated/favicon/apple-touch-icon.png?v=3"
    },
    {
      "url": "https://docs.tavily.com/documentation/about",
      "raw_content": "Who are we?\n-----------\n\nWe're a team of AI researchers and developers passionate about helping you build the next generation of AI assistants. Our mission is to empower individuals and organizations with accurate, unbiased, and factual information.\n\nWhat is the Tavily Search Engine?\n---------------------------------\n\nBuilding an AI agent that leverages realtime online information is not a simple task. Scraping doesn't scale and requires expertise to refine, current search engine APIs don't provide explicit information to queries but simply potential related articles (which are not always related), and are not very customziable for AI agent needs. This is why we're excited to introduce the first search engine for AI agents - [Tavily](https://app.tavily.com/).\n\nTavily is a search engine optimized for LLMs, aimed at efficient, quick and persistent search results. Unlike other search APIs such as Serp or Google, Tavily focuses on optimizing search for AI developers and autonomous AI agents. We take care of all the burden of searching, scraping, filtering and extracting the most relevant information from online sources. All in a single API call!\n\nTo try the API in action, you can now use our hosted version on our [API Playground](https://app.tavily.com/playground).\n\nWhy choose Tavily?\n------------------\n\nTavily shines where others fail, with a Search API optimized for LLMs.\n\nHow does the Search API work?\n-----------------------------\n\nTraditional search APIs suchs as Google, Serp and Bing retrieve search results based on a user query. However, the results are sometimes irrelevant to the goal of the search, and return simple URLs and snippets of content which are not always relevant. Because of this, any developer would need to then scrape the sites to extract relevant content, filter irrelevant information, optimize the content to fit LLM context limits, and more. This task is a burden and requires a lot of time and effort to complete. The Tavily Search API takes care of all of this for you in a single API call.\n\nThe Tavily Search API aggregates up to 20 sites per a single API call, and uses proprietary AI to score, filter and rank the top most relevant sources and content to your task, query or goal. In addition, Tavily allows developers to add custom fields such as context and limit response tokens to enable the optimal search experience for LLMs.\n\nTavily can also help your AI agent make better decisions by including a short answer for cross-agent communication.\n\nGetting started\n---------------\n\n[Sign up](https://app.tavily.com/) for Tavily to get your API key. You get **1,000 free API Credits every month**. No credit card required.\n\n[Get your free API key --------------------- You get 1,000 free API Credits every month. **No credit card required.**](https://app.tavily.com/)Head to our [API Playground](https://app.tavily.com/playground) to familiarize yourself with our API.\n\nTo get started with Tavily's APIs and SDKs using code, head to our [Quickstart Guide](https://docs.tavily.com/guides/quickstart) and follow the steps.",
      "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/tavilyai/_generated/favicon/apple-touch-icon.png?v=3"
    }
  ],
  "response_time": 1.23
}

const { tavily } = require("@tavily/core");

const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.crawl("https://docs.tavily.com", { instructions: "Find all pages on the Python SDK" });

console.log(response);
{
  "base_url": "docs.tavily.com",
  "results": [
    {
      "url": "https://docs.tavily.com/welcome",
      "raw_content": "Welcome - Tavily Docs\n\n[Tavily Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/dark.svg)](https://tavily.com/)\n\nSearch or ask...\n\nCtrl K\n\n- [Support](mailto:support@tavily.com)\n- [Get an API key](https://app.tavily.com)\n- [Get an API key](https://app.tavily.com)\n\nSearch...\n\nNavigation\n\n[Home](/welcome)[Documentation](/documentation/about)[SDKs](/sdk/python/quick-start)[Examples](/examples/use-cases/data-enrichment)[FAQ](/faq/faq)\n\nExplore our docs\n\nYour journey to state-of-the-art web search starts right here.\n\n[## Quickstart\n\nStart searching with Tavily in minutes](documentation/quickstart)[## API Reference\n\nStart using Tavily's powerful APIs](documentation/api-reference/endpoint/search)[## API Credits Overview\n\nLearn how to get and manage your Tavily API Credits](documentation/api-credits)[## Rate Limits\n\nLearn about Tavily's API rate limits for both development and production environments](documentation/rate-limits)[## Python\n\nGet started with our Python SDK, `tavily-python`](sdk/python/quick-start)[## Playground\n\nExplore Tavily's APIs with our interactive playground](https://app.tavily.com/playground)",
      "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/tavilyai/_generated/favicon/apple-touch-icon.png?v=3"
    },
    {
      "url": "https://docs.tavily.com/documentation/api-credits",
      "raw_content": "Credits & Pricing - Tavily Docs\n\n[Tavily Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/tavilyai/logo/dark.svg)](https://tavily.com/)\n\nSearch or ask...\n\nCtrl K\n\n- [Support](mailto:support@tavily.com)\n- [Get an API key](https://app.tavily.com)\n- [Get an API key](https://app.tavily.com)\n\nSearch...\n\nNavigation\n\nOverview\n\nCredits & Pricing\n\n[Home](/welcome)[Documentation](/documentation/about)[SDKs](/sdk/python/quick-start)[Examples](/examples/use-cases/data-enrichment)[FAQ](/faq/faq)\n\n- [API Playground](https://app.tavily.com/playground)\n- [Community](https://community.tavily.com)\n- [Blog](https://blog.tavily.com)\n\n##### Overview\n\n- [About](/documentation/about)\n- [Quickstart](/documentation/quickstart)\n- [Credits & Pricing](/documentation/api-credits)\n- [Rate Limits](/documentation/rate-limits)\n\n##### API Reference\n\n- [Introduction](/documentation/api-reference/introduction)\n- [POST\n\n  Tavily Search](/documentation/api-reference/endpoint/search)\n- [POST\n\n  Tavily Extract](/documentation/api-reference/endpoint/extract)\n- [POST\n\n  Tavily Crawl](/documentation/api-reference/endpoint/crawl)\n- [POST\n\n  Tavily Map](/documentation/api-reference/endpoint/map)\n\n##### Best Practices\n\n- [Best Practices for Search](/documentation/best-practices/best-practices-search)\n- [Best Practices for Extract](/documentation/best-practices/best-practices-extract)\n\n##### Tavily MCP Server\n\n- [Tavily MCP Server](/documentation/mcp)\n\n##### Integrations\n\n- [LangChain](/documentation/integrations/langchain)\n- [LlamaIndex](/documentation/integrations/llamaindex)\n- [Zapier](/documentation/integrations/zapier)\n- [Dify](/documentation/integrations/dify)\n- [Composio](/documentation/integrations/composio)\n- [Make](/documentation/integrations/make)\n- [Agno](/documentation/integrations/agno)\n- [Pydantic AI](/documentation/integrations/pydantic-ai)\n- [FlowiseAI](/documentation/integrations/flowise)\n\n##### Legal\n\n- [Security & Compliance](https://trust.tavily.com)\n- [Privacy Policy](https://tavily.com/privacy)\n\n##### Help\n\n- [Help Center](https://help.tavily.com)\n\n##### Tavily Search Crawler\n\n- [Tavily Search Crawler](/documentation/search-crawler)\n\nOverview\n\n# Credits & Pricing\n\nLearn how to get and manage your Tavily API Credits.\n\n## [\u8203 ](#free-api-credits) Free API Credits\n\n[## Get your free API key\n\nYou get 1,000 free API Credits every month.\n**No credit card required.**](https://app.tavily.com)\n\n## [\u8203 ](#pricing-overview) Pricing Overview\n\nTavily operates on a simple, credit-based model:\n\n- **Free**: 1,000 credits/month\n- **Pay-as-you-go**: $0.008 per credit (allows you to be charged per credit once your plan's credit limit is reached).\n- **Monthly plans**: $0.0075 - $0.005 per credit\n- **Enterprise**: Custom pricing and volume\n\n| **Plan** | **Credits per month** | **Monthly price** | **Price per credit** |\n| --- | --- | --- | --- |\n| **Researcher** | 1,000 | Free | - |\n| **Project** | 4,000 | $30 | $0.0075 |\n| **Bootstrap** | 15,000 | $100 | $0.0067 |\n| **Startup** | 38,000 | $220 | $0.0058 |\n| **Growth** | 100,000 | $500 | $0.005 |\n| **Pay as you go** | Per usage | $0.008 / Credit | $0.008 |\n| **Enterprise** | Custom | Custom | Custom |\n\nHead to [my plan](https://app.tavily.com/account/plan) to explore our different options and manage your plan.\n\n## [\u8203 ](#api-credits-costs) API Credits Costs\n\n### [\u8203 ](#tavily-search) Tavily Search\n\nYour [search depth](/api-reference/endpoint/search#body-search-depth) determines the cost of your request.\n\n- **Basic Search (`basic`):**\n  Each request costs **1 API credit**.\n- **Advanced Search (`advanced`):**\n  Each request costs **2 API credits**.\n\n### [\u8203 ](#tavily-extract) Tavily Extract\n\nThe number of successful URL extractions and your [extraction depth](/api-reference/endpoint/extract#body-extract-depth) determines the cost of your request. You never get charged if a URL extraction fails.\n\n- **Basic Extract (`basic`):**\n  Every 5 successful URL extractions cost **1 API credit**.\n- **Advanced Extract (`advanced`):**\n  Every 5 successful URL extractions cost **2 API credits**.\n\n[Quickstart](/documentation/quickstart)[Rate Limits](/documentation/rate-limits)\n\n[x](https://x.com/tavilyai)[github](https://github.com/tavily-ai)[linkedin](https://linkedin.com/company/tavily)[website](https://tavily.com)\n\n[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=docs&utm_source=docs.tavily.com)\n\nOn this page\n\n- [Free API Credits](#free-api-credits)\n- [Pricing Overview](#pricing-overview)\n- [API Credits Costs](#api-credits-costs)\n- [Tavily Search](#tavily-search)\n- [Tavily Extract](#tavily-extract)",
      "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/tavilyai/_generated/favicon/apple-touch-icon.png?v=3"
    },
    {
      "url": "https://docs.tavily.com/documentation/about",
      "raw_content": "Who are we?\n-----------\n\nWe're a team of AI researchers and developers passionate about helping you build the next generation of AI assistants. Our mission is to empower individuals and organizations with accurate, unbiased, and factual information.\n\nWhat is the Tavily Search Engine?\n---------------------------------\n\nBuilding an AI agent that leverages realtime online information is not a simple task. Scraping doesn't scale and requires expertise to refine, current search engine APIs don't provide explicit information to queries but simply potential related articles (which are not always related), and are not very customziable for AI agent needs. This is why we're excited to introduce the first search engine for AI agents - [Tavily](https://app.tavily.com/).\n\nTavily is a search engine optimized for LLMs, aimed at efficient, quick and persistent search results. Unlike other search APIs such as Serp or Google, Tavily focuses on optimizing search for AI developers and autonomous AI agents. We take care of all the burden of searching, scraping, filtering and extracting the most relevant information from online sources. All in a single API call!\n\nTo try the API in action, you can now use our hosted version on our [API Playground](https://app.tavily.com/playground).\n\nWhy choose Tavily?\n------------------\n\nTavily shines where others fail, with a Search API optimized for LLMs.\n\nHow does the Search API work?\n-----------------------------\n\nTraditional search APIs suchs as Google, Serp and Bing retrieve search results based on a user query. However, the results are sometimes irrelevant to the goal of the search, and return simple URLs and snippets of content which are not always relevant. Because of this, any developer would need to then scrape the sites to extract relevant content, filter irrelevant information, optimize the content to fit LLM context limits, and more. This task is a burden and requires a lot of time and effort to complete. The Tavily Search API takes care of all of this for you in a single API call.\n\nThe Tavily Search API aggregates up to 20 sites per a single API call, and uses proprietary AI to score, filter and rank the top most relevant sources and content to your task, query or goal. In addition, Tavily allows developers to add custom fields such as context and limit response tokens to enable the optimal search experience for LLMs.\n\nTavily can also help your AI agent make better decisions by including a short answer for cross-agent communication.\n\nGetting started\n---------------\n\n[Sign up](https://app.tavily.com/) for Tavily to get your API key. You get **1,000 free API Credits every month**. No credit card required.\n\n[Get your free API key --------------------- You get 1,000 free API Credits every month. **No credit card required.**](https://app.tavily.com/)Head to our [API Playground](https://app.tavily.com/playground) to familiarize yourself with our API.\n\nTo get started with Tavily's APIs and SDKs using code, head to our [Quickstart Guide](https://docs.tavily.com/guides/quickstart) and follow the steps.",
      "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/tavilyai/_generated/favicon/apple-touch-icon.png?v=3"
    }
  ],
  "response_time": 1.23
}

API Reference Tavily Map (BETA) Tavily Map traverses websites like a graph and can explore hundreds of paths in parallel with intelligent discovery to generate comprehensive site maps. POST / map Authorizations Authorization stringheaderrequired Bearer authentication header in the form Bearer <token>, where <token> is your Tavily API key (e.g., Bearer tvly-YOUR_API_KEY). Body application/json Parameters for the Tavily Map request. url stringrequired The root URL to begin the mapping. Example: "docs.tavily.com" max_depth integerdefault:1 Max depth of the mapping. Defines how far from the base URL the crawler can explore. Required range: x >= 1 max_breadth integerdefault:20 Max number of links to follow per level of the tree (i.e., per page). Required range: x >= 1 limit integerdefault:50 Total number of links the crawler will process before stopping. Required range: x >= 1 instructions string Natural language instructions for the crawler. When specified, the cost increases to 2 API credits per 10 successful pages instead of 1 API credit per 10 pages. Example: "Python SDK" select_paths string[] Regex patterns to select only URLs with specific path patterns (e.g., /docs/.*, /api/v1.*). select_domains string[] Regex patterns to select crawling to specific domains or subdomains (e.g., ^docs\\.example\\.com$). exclude_paths string[] Regex patterns to exclude URLs with specific path patterns (e.g., /private/.*, /admin/.*). exclude_domains string[] Regex patterns to exclude specific domains or subdomains from crawling (e.g., ^private\\.example\\.com$). allow_external booleandefault:false Whether to allow following links that go to external domains. categories enum<string>[] Filter URLs using predefined categories like 'Documentation', 'Blog', 'API', etc. Show child attributes Response 200 200 application/json Map results returned successfully base_url string The base URL that was mapped. Example: "docs.tavily.com" results string[] A list of URLs that were discovered during the mapping. Example: [ "https://docs.tavily.com/welcome", "https://docs.tavily.com/documentation/api-credits", "https://docs.tavily.com/documentation/about" ] response_time number Time in seconds it took to complete the request. Example:

const { tavily } = require("@tavily/core");

const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.map("https://docs.tavily.com");

console.log(response);

{
  "base_url": "docs.tavily.com",
  "results": [
    "https://docs.tavily.com/welcome",
    "https://docs.tavily.com/documentation/api-credits",
    "https://docs.tavily.com/documentation/about"
  ],
  "response_time": 1.23
}

API Reference Usage Get API key and account usage details GET / usage Authorizations Authorization stringheaderrequired Bearer authentication header in the form Bearer <token>, where <token> is your Tavily API key (e.g., Bearer tvly-YOUR_API_KEY). Response 200 200 application/json Usage details returned successfully key object Hide child attributes key.usage integer Current usage count for the API key Example: 150 key.limit integer Usage limit for the API key. Returns null if unlimited (2147483647) Example: 1000 account object Account plan and usage information

const options = {method: 'GET', headers: {Authorization: 'Bearer <token>'}};

fetch('https://api.tavily.com/usage', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));

{
  "key": {
    "usage": 150,
    "limit": 1000
  },
  "account": {
    "current_plan": "Bootstrap",
    "plan_usage": 500,
    "plan_limit": 15000,
    "paygo_usage": 25,
    "paygo_limit": 100
  }
}

Best Practices Best Practices for Search Learn how to optimize your queries, refine search filters, and leverage advanced parameters for better performance. Optimizing your query 1. Keep your query under 400 characters For efficient processing, keep your query concise—under 400 characters. Think of it as a query for an agent performing web search, not long-form prompts. If your query exceeds the limit, you’ll see this error: Copy Ask AI { "detail": { "error": "Query is too long. Max query length is 400 characters." } } 2. Break your query into smaller sub-queries If your query is complex or covers multiple topics, consider breaking it into smaller, more focused sub-queries and sending them as separate requests. Good Bad // Breaking the query into smaller, more focused sub-queries. { "query":"Competitors of company ABC." } { "query":"Financial performance of company ABC." } { "query":"Recent developments of company ABC." } { "query":"Latest industry trends related to ABC." } Optimizing your request parameters max_results (Limiting the number of results) Limits the number of search results (default is 5). Good Bad // Customizing max_results based on your needs, limiting the results to 10 to improve relevance and focus on the most relevant sources. { "query": "Info about renewable energy technologies", "max_results": 10 } content (NLP-based snippet) Provides a summarized content snippet. Helps in quickly understanding the main context without extracting full content. When search_depth is set to advanced , it extracts content closely aligned with your query, surfacing the most valuable sections of a web page rather than a generic summary. Additionally, it uses chunks_per_source to determine the number of content chunks to return per source. search_depth=advanced (Ideal for higher relevance in search results) Retrieves the most relevant content snippets for your query. By setting include_raw_content to true, you can increase the likelihood of enhancing retrieval precision and retrieving the desired number of chunks_per_source. Good Good Bad // Using search_depth=advanced and chunks_per_source for a query to get the most relevant content, and enabling include_raw_content. { "query": "How many countries use Monday.com?", "search_depth": "advanced", "chunks_per_source": 3, "include_raw_content": true } time_range (Filtering by Date) Restricts search results to a specific time frame. Good Moderate // Using time_range to filter sources from the past month. { "query": "latest trends in machine learning", "time_range": "month" } include_raw_content (Extracted web content) Set to true to return the full extracted content of the web page, useful for deeper content analysis. However, the most recommended approach for extracting web page content is using a two-step process: Search: Retrieve relevant URLs. Extract: Extract content from those URLs. For more information on this two-step process, please refer to the Best Practices for the Extract API. Good Moderate // Using include_raw_content to retrieve full content for comprehensive analysis. { "query": "The impact of AI in healthcare", "include_raw_content": true } topic=news (Filtering news sources) Limits results to news-related sources. Includes published_date metadata. days specifies the number of days back from the current date to include in the results. The default is 3. Useful for getting real-time updates, particularly about politics, sports, and major current events covered by mainstream media sources. Good Bad // Using "topic=news" and "days=1" to get the latest updates from news sources. { "query": "What happened today in NY?", "topic": "news", "days": 1 } include_domains (Restricting searches to specific domains) Limits searches to predefined trusted domains. Good Bad // Using include_domains to restrict search for more domain-specific information. { "query": "What is the professional background of the CEO at Google?", "include_domains": ["linkedin.com/in"] } Minimize the number of domains in the include_domains list and make sure they are relevant to your search query. Good Bad // Using a concise list of 3 relevant domains to refine search results effectively. { "query": "What are the latest funding rounds for AI startups?", "include_domains": [ "crunchbase.com", "techcrunch.com", "pitchbook.com" ] } exclude_domains (Excluding specific domains) Filters out results from specific domains. Good Bad // Excluding unrelated domains to US economy trends, ensuring that irrelevant sources are filtered out. { "query": "US economy trends in 2025", "exclude_domains": ["espn.com","vogue.com"] } Minimize the number of domains in the exclude_domains list to ensure you only exclude domains that are truly irrelevant to your query. Good Bad // Using a concise list of 3 domains to exclude from the search results. { "query": "US fashion trends in 2025", "exclude_domains": ["nytimes.com","forbes.com","bloomberg.com"] } Controlling search results by website region Example: Limit to U.S.-based websites (.com domain): Copy Ask AI { "query": "latest AI research", "include_domains": ["*.com"] } Example: Exclude Icelandic websites (.is domain): Copy Ask AI { "query": "global economic trends", "exclude_domains": ["*.is"] } Combining include and exclude domains Restrict search to .com but exclude example.com: Copy Ask AI { "query": "AI industry news", "include_domains": ["*.com"], "exclude_domains": ["example.com"] } Asynchronous API calls with Tavily Use async/await to ensure non-blocking API requests. Initialize AsyncTavilyClient once and reuse it for multiple requests. Use asyncio.gather for handling multiple queries concurrently. Implement error handling to manage API failures gracefully. Limit concurrent requests to avoid hitting rate limits. Example: Copy Ask AI import asyncio from tavily import AsyncTavilyClient # Initialize Tavily client tavily_client = AsyncTavilyClient("tvly-YOUR_API_KEY") async def fetch_and_gather(): queries = ["latest AI trends", "future of quantum computing"] # Perform search and continue even if one query fails (using return_exceptions=True) try: responses = await asyncio.gather(*(tavily_client.search(q) for q in queries), return_exceptions=True) # Handle responses and print for response in responses: if isinstance(response, Exception): print(f"Search query failed: {response}") else: print(response) except Exception as e: print(f"Error during search queries: {e}") # Run the function asyncio.run(fetch_and_gather()) Optimizing search results with post-processing techniques When working with Tavily’s Search API, refining search results through post-processing techniques can significantly enhance the relevance of the retrieved information. Combining LLMs with Keyword Filtering One of the most effective ways to refine search results is by using a combination of LLMs and deterministic keyword filtering. LLMs can analyze search results in a more contextual and semantic manner, understanding the deeper meaning of the text. Keyword filtering offers a rule-based approach to eliminate irrelevant results based on predefined terms, ensuring a balance between flexibility and precision. How it works By applying keyword filters before or after processing results with an LLM, you can: Remove results that contain specific unwanted terms. Prioritize articles that contain high-value keywords relevant to your use case. Improve efficiency by reducing the number of search results requiring further LLM processing. Utilizing metadata for improved post-processing Tavily’s Search API provides rich metadata that can be leveraged to refine and prioritize search results. By incorporating metadata into post-processing logic, you can improve precision in selecting the most relevant content. Key metadata fields and their Functions title: Helps in identifying articles that are more likely to be relevant based on their headlines. Filtering results by keyword occurrences in the title can improve result relevancy. raw_content: Provides the extracted content from the web page, allowing deeper analysis. If the content does not provide enough information, raw content can be useful for further filtering and ranking. You can also use the Extract API with a two-step extraction process. For more information, see Best Practices for Extract API. score: Represents the relevancy between the query and the retrieved content snippet. Higher scores typically indicate better matches. content: Offers a general summary of the webpage, providing a quick way to gauge relevance without processing the full content. When search_depth is set to advanced, the content is more closely aligned with the query, offering valuable insights. Enhancing post-processing with metadata By leveraging these metadata elements, you can: Sort results based on scores, prioritizing high-confidence matches. Perform additional filtering based on title or content to refine search results. Understanding the score Parameter Tavily assigns a score to each search result, indicating how well the content aligns with the query. This score helps in ranking and selecting the most relevant results. What does the score mean? The score is a numerical measure of relevance between the content and the query. A higher score generally indicates that the result is more relevant to the query. There is no fixed threshold that determines whether a result is useful. The ideal score cutoff depends on the specific use case. Best practices for using scores Set a minimum score threshold to exclude low-relevance results automatically. Analyze the distribution of scores within a search response to adjust thresholds dynamically. Combine similarity scores with other metadata fields (e.g., url, content) to improve ranking strategies. Using regex-based data extraction In addition to leveraging LLMs and metadata for refining search results, Python’s re.search and re.findall methods can play a crucial role in post-processing by allowing you to parse and extract specific data from the raw_content. These methods enable pattern-based filtering and extraction, enhancing the precision and relevance of the processed results. Benefits of using re.search and re.findall Pattern Matching: Both methods are designed to search for specific patterns in text, which is ideal for structured data extraction. Efficiency: These methods help automate the extraction of specific elements from large datasets, improving post-processing efficiency. Flexibility: You can define custom patterns to match a variety of data types, from dates and addresses to keywords and job titles. How they work re.search: Scans the content for the first occurrence of a specified pattern and returns a match object, which can be used to extract specific parts of the text. Example: import re text = "Company: Tavily, Location: New York" match = re.search(r"Location: (\w+)", text) if match: print(match.group(1)) # Output: New York re.findall: Returns a list of all non-overlapping matches of a pattern in the content, making it suitable for extracting multiple instances of a pattern. Example: text = "Contact: john@example.com, support@tavily.com" emails = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", text) print(emails) # Output: ['john@example.com', 'support@tavily.com'] Common use cases for post-processing Content Filtering: Use re.search to identify sections or specific patterns in content (e.g., dates, locations, company names). Data Extraction: Use re.findall to extract multiple instances of specific data points (e.g., phone numbers, emails). Improving Relevance: Apply regex patterns to remove irrelevant content, ensuring that only the most pertinent information remains. By leveraging post-processing techniques such as LLM-assisted filtering, metadata analysis, and score-based ranking, along with regex-based data extraction, you can optimize Tavily’s Search API results for better relevance. Incorporating these methods into your workflow will help you extract high-quality insights tailored to your needs.

### 4.6. Tavily Map API (BETA)

Tavily Map traverses websites like a graph and can explore hundreds of paths in parallel with intelligent discovery to generate comprehensive site maps. `POST /map`

**Authorizations:**
- `Authorization` (string, header, required): Bearer authentication header in the form `Bearer <token>`, where `<token>` is your Tavily API key (e.g., `Bearer tvly-YOUR_API_KEY`).

**Body (`application/json`) Parameters:**
- `url` (string, required): The root URL to begin the mapping. Example: "docs.tavily.com"
- `max_depth` (integer, default: `1`): Max depth of the mapping. Defines how far from the base URL the crawler can explore. Required range: x >= 1.
- `max_breadth` (integer, default: `20`): Max number of links to follow per level of the tree (i.e., per page). Required range: x >= 1.
- `limit` (integer, default: `50`): Total number of links the crawler will process before stopping. Required range: x >= 1.
- `instructions` (string): Natural language instructions for the crawler. When specified, the cost increases to 2 API credits per 10 successful pages instead of 1 API credit per 10 pages. Example: "Python SDK".
- `select_paths` (string[]): Regex patterns to select only URLs with specific path patterns (e.g., `/docs/.*`, `/api/v1.*`).
- `select_domains` (string[]): Regex patterns to select crawling to specific domains or subdomains (e.g., `^docs\.example\.com$`).
- `exclude_paths` (string[]): Regex patterns to exclude URLs with specific path patterns (e.g., `/private/.*`, `/admin/.*`).
- `exclude_domains` (string[]): Regex patterns to exclude specific domains or subdomains from crawling (e.g., `^private\.example\.com$`).
- `allow_external` (boolean, default: `false`): Whether to allow following links that go to external domains.
- `categories` (enum<string>[]): Filter URLs using predefined categories like 'Documentation', 'Blog', 'API', etc.

**Response (200 `application/json`):** Map results returned successfully.
- `base_url` (string): The base URL that was mapped. Example: "docs.tavily.com"
- `results` (string[]): A list of URLs that were discovered during the mapping. Example: `[ "https://docs.tavily.com/welcome", "https://docs.tavily.com/documentation/api-credits", "https://docs.tavily.com/documentation/about" ]`.
- `response_time` (number): Time in seconds it took to complete the request.

**JavaScript SDK Example for Tavily Map:**
```javascript
const { tavily } = require("@tavily/core");

const tvly = tavily({ apiKey: "tvly-YOUR_API_KEY" });
const response = await tvly.map("https://docs.tavily.com");

console.log(response);
```

**Example Response for Tavily Map:**
```json
{
  "base_url": "docs.tavily.com",
  "results": [
    "https://docs.tavily.com/welcome",
    "https://docs.tavily.com/documentation/api-credits",
    "https://docs.tavily.com/documentation/about"
  ],
  "response_time": 1.23
}
```

### 4.7. Tavily Usage API

Get API key and account usage details. `GET /usage`

**Authorizations:**
- `Authorization` (string, header, required): Bearer authentication header in the form `Bearer <token>`, where `<token>` is your Tavily API key (e.g., `Bearer tvly-YOUR_API_KEY`).

**Response (200 `application/json`):** Usage details returned successfully.
- `key` (object):
  - `usage` (integer): Current usage count for the API key. Example: 150.
  - `limit` (integer): Usage limit for the API key. Returns `null` if unlimited (2147483647). Example: 1000.
- `account` (object): Account plan and usage information.

**JavaScript Example for Tavily Usage:**
```javascript
const options = {method: 'GET', headers: {Authorization: 'Bearer <token>'}};

fetch('https://api.tavily.com/usage', options)
  .then(response => response.json())
  .then(response => console.log(response))
  .catch(err => console.error(err));
```

**Example Response for Tavily Usage:**
```json
{
  "key": {
    "usage": 150,
    "limit": 1000
  },
  "account": {
    "current_plan": "Bootstrap",
    "plan_usage": 500,
    "plan_limit": 15000,
    "paygo_usage": 25,
    "paygo_limit": 100
  }
}
```

## 5. Best Practices

### 5.1. Best Practices for Search

Learn how to optimize your queries, refine search filters, and leverage advanced parameters for better performance.

#### 5.1.1. Optimizing Your Query

1. **Keep your query under 400 characters:** For efficient processing, keep your query concise—under 400 characters. Think of it as a query for an agent performing web search, not long-form prompts. If your query exceeds the limit, you’ll see this error: `{ "detail": { "error": "Query is too long. Max query length is 400 characters." } }`

2. **Break your query into smaller sub-queries:** If your query is complex or covers multiple topics, consider breaking it into smaller, more focused sub-queries and sending them as separate requests.

   **Good Example:**
   ```json
   // Breaking the query into smaller, more focused sub-queries.
   { "query":"Competitors of company ABC." }
   { "query":"Financial performance of company ABC." }
   { "query":"Recent developments of company ABC." }
   { "query":"Latest industry trends related to ABC." }
   ```

#### 5.1.2. Optimizing Your Request Parameters

- **`max_results` (Limiting the number of results):** Limits the number of search results (default is 5).

   **Good Example:**
   ```json
   // Customizing max_results based on your needs, limiting the results to 10 to improve relevance and focus on the most relevant sources.
   { "query": "Info about renewable energy technologies", "max_results": 10 }
   ```

- **`content` (NLP-based snippet):** Provides a summarized content snippet. Helps in quickly understanding the main context without extracting full content. When `search_depth` is set to `advanced`, it extracts content closely aligned with your query, surfacing the most valuable sections of a web page rather than a generic summary. Additionally, it uses `chunks_per_source` to determine the number of content chunks to return per source.

- **`search_depth=advanced` (Ideal for higher relevance in search results):** Retrieves the most relevant content snippets for your query. By setting `include_raw_content` to `true`, you can increase the likelihood of enhancing retrieval precision and retrieving the desired number of `chunks_per_source`.

   **Good Example:**
   ```json
   // Using search_depth=advanced and chunks_per_source for a query to get the most relevant content, and enabling include_raw_content.
   { "query": "How many countries use Monday.com?", "search_depth": "advanced", "chunks_per_source": 3, "include_raw_content": true }
   ```

- **`time_range` (Filtering by Date):** Restricts search results to a specific time frame.

   **Good Example:**
   ```json
   // Using time_range to filter sources from the past month.
   { "query": "latest trends in machine learning", "time_range": "month" }
   ```

- **`include_raw_content` (Extracted web content):** Set to `true` to return the full extracted content of the web page, useful for deeper content analysis. However, the most recommended approach for extracting web page content is using a two-step process: Search: Retrieve relevant URLs. Extract: Extract content from those URLs. For more information on this two-step process, please refer to the Best Practices for the Extract API.

   **Good Example:**
   ```json
   // Using include_raw_content to retrieve full content for comprehensive analysis.
   { "query": "The impact of AI in healthcare", "include_raw_content": true }
   ```

- **`topic=news` (Filtering news sources):** Limits results to news-related sources. Includes `published_date` metadata. `days` specifies the number of days back from the current date to include in the results. The default is 3. Useful for getting real-time updates, particularly about politics, sports, and major current events covered by mainstream media sources.

   **Good Example:**
   ```json
   // Using "topic=news" and "days=1" to get the latest updates from news sources.
   { "query": "What happened today in NY?", "topic": "news", "days": 1 }
   ```

- **`include_domains` (Restricting searches to specific domains):** Limits searches to predefined trusted domains.

   **Good Example:**
   ```json
   // Using include_domains to restrict search for more domain-specific information.
   { "query": "What is the professional background of the CEO at Google?", "include_domains": ["linkedin.com/in"] }
   ```
   Minimize the number of domains in the `include_domains` list and make sure they are relevant to your search query.

   **Good Example:**
   ```json
   // Using a concise list of 3 relevant domains to refine search results effectively.
   { "query": "What are the latest funding rounds for AI startups?", "include_domains": [ "crunchbase.com", "techcrunch.com", "pitchbook.com" ] }
   ```

- **`exclude_domains` (Excluding specific domains):** Filters out results from specific domains.

   **Good Example:**
   ```json
   // Excluding unrelated domains to US economy trends, ensuring that irrelevant sources are filtered out.
   { "query": "US economy trends in 2025", "exclude_domains": ["espn.com","vogue.com"] }
   ```
   Minimize the number of domains in the `exclude_domains` list to ensure you only exclude domains that are truly irrelevant to your query.

   **Good Example:**
   ```json
   // Using a concise list of 3 domains to exclude from the search results.
   { "query": "US fashion trends in 2025", "exclude_domains": ["nytimes.com","forbes.com","bloomberg.com"] }
   ```

- **Controlling search results by website region:**

   **Example: Limit to U.S.-based websites (.com domain):**
   ```json
   { "query": "latest AI research", "include_domains": ["*.com"] }
   ```

   **Example: Exclude Icelandic websites (.is domain):**
   ```json
   { "query": "global economic trends", "exclude_domains": ["*.is"] }
   ```

- **Combining `include` and `exclude` domains:** Restrict search to .com but exclude example.com:

   **Example:**
   ```json
   { "query": "AI industry news", "include_domains": ["*.com"], "exclude_domains": ["example.com"] }
   ```

#### 5.1.3. Asynchronous API Calls with Tavily

Use `async/await` to ensure non-blocking API requests. Initialize `AsyncTavilyClient` once and reuse it for multiple requests. Use `asyncio.gather` for handling multiple queries concurrently. Implement error handling to manage API failures gracefully. Limit concurrent requests to avoid hitting rate limits.

**Example:**
```python
import asyncio
from tavily import AsyncTavilyClient

# Initialize Tavily client
tavily_client = AsyncTavilyClient("tvly-YOUR_API_KEY")

async def fetch_and_gather():
    queries = ["latest AI trends", "future of quantum computing"]
    # Perform search and continue even if one query fails (using return_exceptions=True)
    try:
        responses = await asyncio.gather(*(tavily_client.search(q) for q in queries), return_exceptions=True)
        # Handle responses and print
        for response in responses:
            if isinstance(response, Exception):
                print(f"Search query failed: {response}")
            else:
                print(response)
    except Exception as e:
        print(f"Error during search queries: {e}")

# Run the function
asyncio.run(fetch_and_gather())
```

#### 5.1.4. Optimizing Search Results with Post-Processing Techniques

When working with Tavily’s Search API, refining search results through post-processing techniques can significantly enhance the relevance of the retrieved information.

**Combining LLMs with Keyword Filtering:** One of the most effective ways to refine search results is by using a combination of LLMs and deterministic keyword filtering. LLMs can analyze search results in a more contextual and semantic manner, understanding the deeper meaning of the text. Keyword filtering offers a rule-based approach to eliminate irrelevant results based on predefined terms, ensuring a balance between flexibility and precision.

**How it works:** By applying keyword filters before or after processing results with an LLM, you can:
- Remove results that contain specific unwanted terms.
- Prioritize articles that contain high-value keywords relevant to your use case.
- Improve efficiency by reducing the number of search results requiring further LLM processing.

**Utilizing Metadata for Improved Post-Processing:** Tavily’s Search API provides rich metadata that can be leveraged to refine and prioritize search results. By incorporating metadata into post-processing logic, you can improve precision in selecting the most relevant content.

**Key metadata fields and their Functions:**
- `title`: Helps in identifying articles that are more likely to be relevant based on their headlines. Filtering results by keyword occurrences in the title can improve result relevancy.
- `raw_content`: Provides the extracted content from the web page, allowing deeper analysis. If the content does not provide enough information, raw content can be useful for further filtering and ranking. You can also use the Extract API with a two-step extraction process. For more information, see Best Practices for Extract API.
- `score`: Represents the relevancy between the query and the retrieved content snippet. Higher scores typically indicate better matches.
- `content`: Offers a general summary of the webpage, providing a quick way to gauge relevance without processing the full content. When `search_depth` is set to `advanced`, the content is more closely aligned with the query, offering valuable insights.

**Enhancing Post-Processing with Metadata:** By leveraging these metadata elements, you can:
- Sort results based on scores, prioritizing high-confidence matches.
- Perform additional filtering based on title or content to refine search results.

**Understanding the `score` Parameter:** Tavily assigns a score to each search result, indicating how well the content aligns with the query. This score helps in ranking and selecting the most relevant results.

**What does the score mean?** The score is a numerical measure of relevance between the content and the query. A higher score generally indicates that the result is more relevant to the query. There is no fixed threshold that determines whether a result is useful. The ideal score cutoff depends on the specific use case.

**Best practices for using scores:**
- Set a minimum score threshold to exclude low-relevance results automatically.
- Analyze the distribution of scores within a search response to adjust thresholds dynamically.
- Combine similarity scores with other metadata fields (e.g., `url`, `content`) to improve ranking strategies.

**Using Regex-Based Data Extraction:** In addition to leveraging LLMs and metadata for refining search results, Python’s `re.search` and `re.findall` methods can play a crucial role in post-processing by allowing you to parse and extract specific data from the `raw_content`. These methods enable pattern-based filtering and extraction, enhancing the precision and relevance of the processed results.

**Benefits of using `re.search` and `re.findall`:**
- **Pattern Matching:** Both methods are designed to search for specific patterns in text, which is ideal for structured data extraction.
- **Efficiency:** These methods help automate the extraction of specific elements from large datasets, improving post-processing efficiency.
- **Flexibility:** You can define custom patterns to match a variety of data types, from dates and addresses to keywords and job titles.

**How they work:**
- `re.search`: Scans the content for the first occurrence of a specified pattern and returns a match object, which can be used to extract specific parts of the text.

   **Example:**
   ```python
   import re
   text = "Company: Tavily, Location: New York"
   match = re.search(r"Location: (\w+)", text)
   if match:
       print(match.group(1))  # Output: New York
   ```

- `re.findall`: Returns a list of all non-overlapping matches of a pattern in the content, making it suitable for extracting multiple instances of a pattern.

   **Example:**
   ```python
   import re
   text = "Contact: john@example.com, support@tavily.com"
   emails = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", text)
   print(emails)  # Output: ['john@example.com', 'support@tavily.com']
   ```

**Common Use Cases for Post-Processing:**
- **Content Filtering:** Use `re.search` to identify sections or specific patterns in content (e.g., dates, locations, company names).
- **Data Extraction:** Use `re.findall` to extract multiple instances of specific data points (e.g., phone numbers, emails).
- **Improving Relevance:** Apply regex patterns to remove irrelevant content, ensuring that only the most pertinent information remains.

By leveraging post-processing techniques such as LLM-assisted filtering, metadata analysis, and score-based ranking, along with regex-based data extraction, you can optimize Tavily’s Search API results for better relevance. Incorporating these methods into your workflow will help you extract high-quality insights tailored to your needs.

### 5.2. Best Practices for Extract

Learn the best practices for web content extraction process. Efficiently extracting content from web pages is crucial for AI-powered applications. Tavily provides two main approaches to content extraction, each suited for different use cases.

#### 5.2.1. One-step Extraction: Directly Retrieve `raw_content`

You can extract web content by enabling `include_raw_content = true` when making a Tavily Search API call. This allows you to retrieve both search results and extracted content in a single step. However, this can increase latency because you may extract raw content from sources that are not relevant in the first place. It’s recommended to split the process into two steps: running multiple sub-queries to expand the pool of sources, then curating the most relevant documents based on content snippets or source scores. By extracting raw content from the most relevant sources, you get high-quality RAG documents.

#### 5.2.2. Two-step Process: Search, Then Extract

For better accuracy and customization, we recommend a two-step process:

**Step 1: Search**
Use the Tavily Search API to retrieve relevant web pages, which output URLs.

**Step 2: Extract**
Use the Tavily Extract API to fetch the full content from the most relevant URLs.

**Example:**
```python
import asyncio
from tavily import AsyncTavilyClient

tavily_client = AsyncTavilyClient(api_key="tvly-YOUR_API_KEY")

async def fetch_and_extract():
    # Define the queries with search_depth and max_results inside the query dictionary
    queries = [
        {"query": "AI applications in healthcare", "search_depth": "advanced", "max_results": 10},
        {"query": "ethical implications of AI in healthcare", "search_depth": "advanced", "max_results": 10},
        {"query": "latest trends in machine learning healthcare applications", "search_depth": "advanced", "max_results": 10},
        {"query": "AI and healthcare regulatory challenges", "search_depth": "advanced", "max_results": 10}
    ]
    # Perform the search queries concurrently, passing the entire query dictionary
    responses = await asyncio.gather(*[tavily_client.search(**q) for q in queries])

    # Filter URLs with a score greater than 0.5. Alternatively, you can use a re-ranking model or an LLM to identify the most relevant sources, or cluster your documents and extract content only from the most relevant cluster
    relevant_urls = []
    for response in responses:
        for result in response.get('results', []):
            if result.get('score', 0) > 0.5:
                relevant_urls.append(result.get('url'))

    # Extract content from the relevant URLs
    extracted_data = await asyncio.gather(*(tavily_client.extract(url) for url in relevant_urls))

    # Print the extracted content
    for data in extracted_data:
        print(data)

# Run the function
asyncio.run(fetch_and_gather())
```

**Pros of Two-Step Extraction:**
- More control – Extract only from selected URLs.
- Higher accuracy – Filter out irrelevant results before extraction.
- Advanced extraction capabilities – Using `search_depth = "advanced"`.

**Cons of Two-Step Extraction:**
- Slightly more expensive.

**Using Advanced Extraction:** Using `extract_depth = "advanced"` in the Extract API allows for more comprehensive content retrieval. This mode is particularly useful when dealing with:
- Complex web pages with dynamic content, embedded media, or structured data.
- Tables and structured information that require accurate parsing.
- Higher success rates.

If precision and depth are priorities for your application, `extract_depth = "advanced"` is the recommended choice.

### 5.3. Best Practices for Crawl

Learn how to effectively use Tavily’s Crawl API to extract and process web content.

#### 5.3.1. When to Use Crawl vs. Map

**Use Crawl when you need:**
- Full content extraction from pages
- Deep content analysis
- Processing of paginated or nested content
- Extraction of specific content patterns
- Integration with RAG systems

**Use Map when you need:**
- Quick site structure discovery
- URL collection without content extraction
- Sitemap generation
- Path pattern matching
- Domain structure analysis

#### 5.3.2. Use Cases for Crawl

1. **Deep or Unlinked Content:** Many sites have content that’s difficult to access through standard means:
   - Deeply nested pages not in main navigation
   - Paginated archives (old blog posts, changelogs)
   - Internal search-only content

   **Best Practice:**
   ```json
   { "url": "example.com", "max_depth": 3, "max_breadth": 50, "limit": 200, "select_paths": ["/blog/.*", "/changelog/.*"], "exclude_paths": ["/private/.*", "/admin/.*"] }
   ```

2. **Structured but Nonstandard Layouts:** For content that’s structured but not marked up in schema.org:
   - Documentation
   - Changelogs
   - FAQs

   **Best Practice:**
   ```json
   { "url": "docs.example.com", "max_depth": 2, "extract_depth": "advanced", "categories": ["Documentation"], "select_paths": ["/docs/.*"] }
   ```

3. **Multi-modal Information Needs:** When you need to combine information from multiple sections:
   - Cross-referencing content
   - Finding related information
   - Building comprehensive knowledge bases

   **Best Practice:**
   ```json
   { "url": "example.com", "max_depth": 2, "instructions": "Find all documentation pages that link to API reference docs", "extract_depth": "advanced" }
   ```

4. **Rapidly Changing Content:** For content that updates frequently:
   - API documentation
   - Product announcements
   - News sections

   **Best Practice:**
   ```json
   { "url": "api.example.com", "max_depth": 1, "max_breadth": 100, "extract_depth": "basic" }
   ```

5. **Behind Auth / Paywalls:** For content requiring authentication:
   - Internal knowledge bases
   - Customer help centers
   - Gated documentation

   **Best Practice:**
   ```json
   { "url": "help.example.com", "max_depth": 2, "select_domains": ["^help\\.example\\.com$"], "exclude_domains": ["^private\\.example\\.com$"] }
   ```

6. **Complete Coverage / Auditing:** For comprehensive content analysis:
   - Legal compliance checks
   - Security audits
   - Policy verification

   **Best Practice:**
   ```json
   { "url": "example.com", "max_depth": 3, "max_breadth": 100, "limit": 1000, "extract_depth": "advanced", "instructions": "Find all mentions of GDPR and data protection policies" }
   ```

7. **Semantic Search or RAG Integration:** For feeding content into LLMs or search systems:
   - RAG systems
   - Enterprise search
   - Knowledge bases

   **Best Practice:**
   ```json
   { "url": "docs.example.com", "max_depth": 2, "extract_depth": "advanced", "include_images": true, "categories": ["Documentation", "API"] }
   ```

8. **Known URL Patterns:** When you have specific paths to crawl:
   - Sitemap-based crawling
   - Section-specific extraction
   - Pattern-based content collection

   **Best Practice:**
   ```json
   { "url": "example.com", "max_depth": 1, "select_paths": ["/docs/.*", "/api/.*", "/guides/.*"], "exclude_paths": ["/private/.*", "/admin/.*"] }
   ```

#### 5.3.3. Performance Considerations for Crawl

- **Depth vs. Performance:** Each level of depth increases crawl time exponentially. Start with `max_depth: 1` and increase as needed. Use `max_breadth` to control horizontal expansion. Set appropriate `limit` to prevent excessive crawling.

- **Resource Optimization:** Use `basic extract_depth` for simple content. Use `advanced extract_depth` only when needed. Set appropriate `max_breadth` based on site structure. Use `select_paths` and `exclude_paths` to focus crawling.

- **Rate Limiting:** Respect site’s `robots.txt`. Implement appropriate delays between requests. Monitor API usage and limits. Use appropriate error handling for rate limits.

#### 5.3.4. Best Practices Summary for Crawl

- **Start Small:** Begin with limited depth and breadth. Gradually increase based on needs. Monitor performance and adjust.
- **Be Specific:** Use path patterns to focus crawling. Exclude irrelevant sections. Set appropriate categories.
- **Optimize Resources:** Choose appropriate `extract_depth`. Set reasonable limits. Use `include_images` only when needed.
- **Handle Errors:** Implement retry logic. Monitor failed results. Handle rate limits appropriately.
- **Security:** Respect `robots.txt`. Use appropriate authentication. Exclude sensitive paths.
- **Integration:** Plan for data processing. Consider storage requirements. Design for scalability.

#### 5.3.5. Common Pitfalls for Crawl

- **Excessive Depth:** Avoid setting `max_depth` too high. Start with 1-2 levels. Increase only if necessary.
- **Unfocused Crawling:** Set appropriate categories. Use instructions for guidance.
- **Resource Overuse:** Monitor API usage. Set appropriate limits. Use `basic extract_depth` when possible.
- **Missing Content:** Verify path patterns. Monitor crawl coverage.

#### 5.3.6. Integration with Map

Consider using Map before Crawl to:
- Discover site structure
- Identify relevant paths
- Plan crawl strategy
- Validate URL patterns

**Example workflow:**
- Use Map to get site structure
- Analyze paths and patterns
- Configure Crawl with discovered paths
- Execute focused crawl

#### 5.3.7. Conclusion for Crawl

Tavily’s Crawl API is powerful for extracting structured content from websites. By following these best practices, you can:
- Optimize crawl performance
- Ensure complete coverage
- Maintain resource efficiency
- Build robust content extraction pipelines

Remember to:
- Start with limited scope
- Use appropriate parameters
- Monitor performance
- Handle errors gracefully
- Respect site policies

